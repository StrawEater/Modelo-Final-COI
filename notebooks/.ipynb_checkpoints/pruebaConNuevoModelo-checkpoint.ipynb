{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc878647-50d9-4032-9df0-ef5f3a7f393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/data3/junibg-ego/Modelo_leo_coi\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5add3976-9b2f-4c8c-a961-801ad4af4930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from triton==3.4.0->torch) (80.9.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from triton==3.4.0->torch) (8.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from importlib-metadata->triton==3.4.0->torch) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: transformers in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from requests->transformers) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: ipywidgets in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack-data in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: seaborn in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from seaborn) (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.23.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Requirement already satisfied: einops in /home/juanibg/micromamba/envs/investigathon_3.9/lib/python3.9/site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install ipywidgets\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d09de48-c800-4593-934e-35be9e68a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¿GPU disponible? True\n",
      "NÃºmero de GPUs: 2\n",
      "Nombre GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Â¿GPU disponible? {torch.cuda.is_available()}\")\n",
    "print(f\"NÃºmero de GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"Nombre GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53085c88-4b09-476c-a880-252cfa5fdc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Tus imports\n",
    "from src.combined_model.combined_model_embedding import *\n",
    "from src.combined_model.combined_models import *\n",
    "from src.decoders.decoder_simple import *\n",
    "from src.encoders_model.DNABERT_Embedder import *\n",
    "from src.encoders_model.embdeeding_encoders import *\n",
    "from src.encoders_model.simple_encoders import *\n",
    "from src.evaluators.linear_evaluator import *\n",
    "from src.decoders.sequence_decoder import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67f6fe74-ed42-42cc-8071-39b1110dee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.encoders_model.simple_encoders import *\n",
    "from src.utils.load_fastaDataset import *\n",
    "from src.training.experimentRunner import *\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11a0540b-2f1b-4835-b2af-af5a0fd105d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hierarchy_from_json(json_path):\n",
    "    \"\"\"Carga la jerarquÃ­a taxonÃ³mica desde JSON\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        hierarchy_raw = json.load(f)\n",
    "    \n",
    "    # Convertir claves string a int\n",
    "    hierarchy = {}\n",
    "    for child_taxon, parent_dict in hierarchy_raw.items():\n",
    "        hierarchy[child_taxon] = {}\n",
    "        for parent_key, children_list in parent_dict.items():\n",
    "            parent_int = int(float(parent_key))\n",
    "            children_int = [int(c) for c in children_list]\n",
    "            hierarchy[child_taxon][parent_int] = children_int\n",
    "    \n",
    "    print(\"âœ… JerarquÃ­a cargada desde JSON\")\n",
    "    for taxon, mapping in hierarchy.items():\n",
    "        n_parents = len(mapping)\n",
    "        n_children = sum(len(v) for v in mapping.values())\n",
    "        print(f\"  {taxon:10s}: {n_parents:4d} padres â†’ {n_children:5d} hijos\")\n",
    "    \n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "763948cf-2509-449a-9a3a-7acea1040f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… JerarquÃ­a cargada desde JSON\n",
      "  class     :   49 padres â†’   187 hijos\n",
      "  order     :  173 padres â†’   831 hijos\n",
      "  family    :  797 padres â†’  5446 hijos\n",
      "  genus     : 5393 padres â†’ 50568 hijos\n",
      "  species   : 50510 padres â†’ 205075 hijos\n"
     ]
    }
   ],
   "source": [
    "hierarchy_path = os.path.join(project_root, \"src\", \"data\", \"taxonomy_hierarchy_fixed_with_class.json\")\n",
    "hierarchy = load_hierarchy_from_json(hierarchy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ea34314-0a7a-4eca-9ccc-b8920d224aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = os.path.join(project_root, \"src\", \"data\", \"all_taxa_numeric.csv\")\n",
    "df = pd.read_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adc3d09b-ef72-4697-aee2-07c26dd76a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  phylum    :     49 clases\n",
      "  class     :    173 clases\n",
      "  order     :    797 clases\n",
      "  family    :   5393 clases\n",
      "  genus     :  50510 clases\n",
      "  species   : 205075 clases\n"
     ]
    }
   ],
   "source": [
    "taxon_order = ['phylum', 'class','order', 'family', 'genus', 'species']\n",
    "total_classes = {}\n",
    "for taxon in taxon_order:\n",
    "    n_classes = df[taxon].nunique()\n",
    "    total_classes[taxon] = n_classes\n",
    "    print(f\"  {taxon:10s}: {n_classes:6d} clases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d341ae25-9f15-4697-881e-307b53dc82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Primero separar test (20%)\n",
    "df_temp, df_test = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['phylum']\n",
    ")\n",
    "\n",
    "# Luego separar train/val (80/20 del 80% restante = 64/16 del total)\n",
    "df_train, df_val = train_test_split(\n",
    "    df_temp, test_size=0.2, random_state=42, stratify=df_temp['phylum']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3e34dfa-f51b-4fcb-833a-c8ee558f799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 750\n",
    "\n",
    "train_dataset = MultiTaxaFastaDataset(\n",
    "    df_train.reset_index(drop=True), \n",
    "    max_length=max_length,\n",
    "    taxon_cols=taxon_order\n",
    ")\n",
    "\n",
    "val_dataset = MultiTaxaFastaDataset(\n",
    "    df_val.reset_index(drop=True),\n",
    "    max_length=max_length,\n",
    "    taxon_cols=taxon_order\n",
    ")\n",
    "\n",
    "test_dataset = MultiTaxaFastaDataset(\n",
    "    df_test.reset_index(drop=True),\n",
    "    max_length=max_length,\n",
    "    taxon_cols=taxon_order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d66e91b-3e20-48e9-bdb3-cc09b9d92b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_multitask(batch, taxon_cols=['phylum', 'class','order','family','genus','species'], max_length=900):\n",
    "    sequences, labels_dict_list, recon_targets_list, true_tokens_list = zip(*batch)\n",
    "\n",
    "    # Labels: dict de tensors\n",
    "    labels_dict = {taxon: torch.stack([d[taxon] for d in labels_dict_list]) for taxon in taxon_cols}\n",
    "\n",
    "    # Recon targets: dict de tensors\n",
    "    recon_targets_dict = {taxon: torch.stack([d[taxon] for d in recon_targets_list]) for taxon in taxon_cols}\n",
    "\n",
    "    # True tokens\n",
    "    true_tokens = torch.stack(true_tokens_list)\n",
    "\n",
    "    return sequences, labels_dict, recon_targets_dict, true_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eddb784-fdef-47be-9b34-9cf0660cb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=lambda b: collate_multitask(b, taxon_cols=val_dataset.taxon_cols, max_length=val_dataset.max_length),\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    collate_fn=lambda b: collate_multitask(b, taxon_cols=val_dataset.taxon_cols, max_length=val_dataset.max_length),\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "test_loader  = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    collate_fn=lambda b: collate_multitask(b, taxon_cols=val_dataset.taxon_cols, max_length=val_dataset.max_length),\n",
    "    num_workers=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45ac734b-d0b9-44ed-bb8a-4238e996c93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸  Dispositivo: cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# DNABERT\u001b[39;00m\n\u001b[32m      7\u001b[39m dnabert_path = os.path.join(project_root, \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marchives\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m dnabert = \u001b[43mDNABERTEmbedder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdnabert_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m embed_dim = dnabert.get_embedding_dim()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/data3/junibg-ego/Modelo_leo_coi/src/encoders_model/DNABERT_Embedder.py:23\u001b[39m, in \u001b[36mDNABERTEmbedder.__init__\u001b[39m\u001b[34m(self, model_name, max_length, device)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.max_length = max_length\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.device = torch.device(device)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_route\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.freeze_network()\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.model.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/data3/junibg-ego/Modelo_leo_coi/src/encoders_model/DNABERT_Embedder.py:34\u001b[39m, in \u001b[36mDNABERTEmbedder.load_from_route\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_route\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(\n\u001b[32m     30\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_name,\n\u001b[32m     31\u001b[39m         trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     32\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:597\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    595\u001b[39m         model_class.register_for_auto_class(auto_class=\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    596\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m    601\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/lib/python3.12/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/lib/python3.12/site-packages/transformers/modeling_utils.py:5316\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5313\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   5314\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5315\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m5316\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   5317\u001b[39m     )\n\u001b[32m   5319\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   5320\u001b[39m prefix = model.base_model_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/lib/python3.12/site-packages/transformers/modeling_utils.py:508\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/lib/python3.12/site-packages/transformers/utils/import_utils.py:1647\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1645\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1646\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1648\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1649\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1650\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1651\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1652\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ–¥ï¸  Dispositivo: {device}\")\n",
    "\n",
    "max_length = 750\n",
    "\n",
    "# DNABERT\n",
    "dnabert_path = os.path.join(project_root, \"src\", \"data\", \"archives\")\n",
    "dnabert = DNABERTEmbedder(\n",
    "    model_name=dnabert_path,\n",
    "    max_length=max_length,\n",
    "    device=device\n",
    ")\n",
    "embed_dim = dnabert.get_embedding_dim()\n",
    "\n",
    "# Encoder\n",
    "latent_dim = 256\n",
    "encoder = SimpleEmbeddingEncoder(\n",
    "    embed_dim=embed_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Decoders (uno por taxÃ³n)\n",
    "decoders_dict = {}\n",
    "for taxon in taxon_order:\n",
    "    decoders_dict[taxon] = SequenceDecoder(\n",
    "        latent_dim=latent_dim,\n",
    "        seq_len=max_length,\n",
    "        vocab_size=4,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "# Global decoder\n",
    "global_decoder = SequenceDecoder(\n",
    "    latent_dim=latent_dim,\n",
    "    seq_len=max_length,\n",
    "    vocab_size=4,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Classifiers (uno por taxÃ³n)\n",
    "classifiers_dict = {}\n",
    "print(f\"\\nğŸ”§ Creando classifiers con nÃºmero TOTAL de clases:\")\n",
    "for taxon in taxon_order:\n",
    "    # âœ… CORRECTO: Usar total_classes (del dataset completo)\n",
    "    # âŒ INCORRECTO: n_classes = df_train[taxon].nunique()\n",
    "    n_classes = total_classes[taxon]\n",
    "    \n",
    "    classifiers_dict[taxon] = CosineClassifier(\n",
    "        latent_dim=latent_dim,\n",
    "        num_classes=n_classes,\n",
    "        scale=20.0  # â­ Ajustable si es necesario\n",
    "    )\n",
    "    print(f\"  {taxon:10s}: {n_classes:6d} clases\")\n",
    "\n",
    "# â­ CREAR MODELO JERÃRQUICO\n",
    "model = HierarchicalCombinedModelFixed(\n",
    "    dnabert=dnabert,\n",
    "    encoder=encoder,\n",
    "    decoders_dict=decoders_dict,\n",
    "    classifiers_dict=classifiers_dict,\n",
    "    global_decoder=global_decoder,\n",
    "    taxonomy_hierarchy=hierarchy  # â­ AquÃ­ usas la jerarquÃ­a\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Modelo jerÃ¡rquico creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd7e926b-8686-4113-8010-ce1f4b8084a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Modo MULTITASK ACTIVADO (clasificaciÃ³n + reconstrucciÃ³n)\n",
      "ğŸ“ Curriculum Learning Adaptativo Inicializado\n",
      "   Target accuracy: 85%\n",
      "   Max epochs por nivel: 2\n"
     ]
    }
   ],
   "source": [
    "runner = ExperimentRunner(model, device=device)\n",
    "\n",
    "TARGET_ACCURACY = 0.85  # 85% para avanzar al siguiente nivel\n",
    "MAX_EPOCHS_PER_LEVEL = 2  # MÃ¡ximo 3 Ã©pocas por nivel\n",
    "TOTAL_EPOCHS = 12  # Total de Ã©pocas\n",
    "\n",
    "\n",
    "curriculum_trainer = AdaptiveCurriculumTrainer(\n",
    "    model=model,\n",
    "    runner=runner,\n",
    "    device=device,\n",
    "    target_accuracy=TARGET_ACCURACY,\n",
    "    max_epochs_per_level=MAX_EPOCHS_PER_LEVEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9907cbe8-b8ea-42fd-b983-62c21ab81504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.6\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /home/juanibg/micromamba/lib/python3.12/site-packages (from torch==2.6) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/juanibg/micromamba/lib/python3.12/site-packages (from torch==2.6) (4.15.0)\n",
      "Requirement already satisfied: networkx in /home/juanibg/micromamba/lib/python3.12/site-packages (from torch==2.6) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/juanibg/micromamba/lib/python3.12/site-packages (from torch==2.6) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/juanibg/micromamba/lib/python3.12/site-packages (from torch==2.6) (2025.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch==2.6)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /home/juanibg/micromamba/lib/python3.12/site-packages (from torch==2.6) (80.9.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.6)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/juanibg/micromamba/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/juanibg/micromamba/lib/python3.12/site-packages (from jinja2->torch==2.6) (3.0.3)\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m  \u001b[33m0:00:17\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m  \u001b[33m0:00:14\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.5.1\n",
      "\u001b[2K    Uninstalling triton-3.5.1:\n",
      "\u001b[2K      Successfully uninstalled triton-3.5.1\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/16\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.7.10/16\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.7.1:â”â”â”â”\u001b[0m \u001b[32m 0/16\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1m 0/16\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: sympy38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:\u001b[38;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]dia-cusparselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.8.9038;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.8.90:114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.8.90;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.8.93â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.937mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu125;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.57mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.5:mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.5237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu122;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.9.905;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.9.90:;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.9.908;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12[38;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.3.8337mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.3.83:7mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:8;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.9038;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12[38;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12m\u001b[38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90â•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12â”â”â”â”\u001b[0m\u001b[38;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.8.4.10m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.8.4.1:m\u001b[38;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.8.9314mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\u001b[38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.10.2.217mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.10.2.21:â”â”\u001b[0m\u001b[38;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.3.90;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.3.90:â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.9049;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torchâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.9.1â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;5;237mâ•º\u001b[0m\u001b[38;5;237mâ”â”â”â”\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.9.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”\u001b[0m \u001b[32m15/16\u001b[0m [torch]u12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.9.1â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[38;2;249;38;114mâ•¸\u001b[0m\u001b[38;5;237mâ”â”\u001b[0m \u001b[32m15/16\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16/16\u001b[0m [torch]8;5;237mâ”â”\u001b[0m \u001b[32m15/16\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c53dddd2-8ace-4fd7-8efb-a2e23fa306c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“ INICIANDO CURRICULUM LEARNING ADAPTATIVO\n",
      "======================================================================\n",
      "ğŸ¯ Threshold objetivo: 85.0%\n",
      "â±ï¸  Ã‰pocas mÃ¡ximas por nivel: 2\n",
      "ğŸ“Š Ã‰pocas mÃ¡ximas totales: 8\n",
      "ğŸ›‘ Early stopping: Activado\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EPOCH 1/8\n",
      "======================================================================\n",
      "ğŸ“š Nivel actual: 1/6 - phylum\n",
      "   Ã‰pocas en este nivel: 1/2\n",
      "   Pesos: {'phylum': 5.0, 'class': 1.0, 'order': 1.0, 'family': 1.0, 'genus': 1.0, 'species': 1.0}\n",
      "ğŸ“Š Pesos por taxÃ³n: {'phylum': 5.0, 'class': 1.0, 'order': 1.0, 'family': 1.0, 'genus': 1.0, 'species': 1.0}\n",
      "ğŸš€ Mixed Precision: True\n",
      "ğŸ“¦ Batches por Ã©poca: 3861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch 1/1:   0%|                                                                                 | 0/3861 [00:00<?, ?it/s]/venv/main/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "[TRAIN] Epoch 1/1:   0%|                                                                                 | 0/3861 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mcurriculum_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_early_stop\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/dna-neural-network-coi-/src/training/experimentRunner.py:664\u001b[39m, in \u001b[36mAdaptiveCurriculumTrainer.train\u001b[39m\u001b[34m(self, train_loader, val_loader, total_epochs, alpha, beta, lr, mixed_precision, enable_early_stop)\u001b[39m\n\u001b[32m    661\u001b[39m all_history[\u001b[33m'\u001b[39m\u001b[33mweights_history\u001b[39m\u001b[33m'\u001b[39m].append(current_weights.copy())\n\u001b[32m    663\u001b[39m \u001b[38;5;66;03m# Entrenar una Ã©poca\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m664\u001b[39m epoch_history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_multitask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Una Ã©poca a la vez\u001b[39;49;00m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtaxon_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Desactivar early stopping interno\u001b[39;49;00m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_best\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No guardar cada Ã©poca\u001b[39;49;00m\n\u001b[32m    675\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[38;5;66;03m# Guardar mÃ©tricas\u001b[39;00m\n\u001b[32m    678\u001b[39m all_history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(epoch_history[\u001b[33m'\u001b[39m\u001b[33mtrain_total\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/dna-neural-network-coi-/src/training/experimentRunner.py:143\u001b[39m, in \u001b[36mExperimentRunner.train_multitask\u001b[39m\u001b[34m(self, train_loader, val_loader, num_epochs, alpha, beta, lr, taxon_weights, mixed_precision, early_stopping_patience, save_best)\u001b[39m\n\u001b[32m    140\u001b[39m optimizer.zero_grad()\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, dtype=torch.float16, enabled=mixed_precision):\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_hierarchy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     logits_dict = out[\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    145\u001b[39m     recon_dict = out[\u001b[33m\"\u001b[39m\u001b[33mrecon\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/dna-neural-network-coi-/src/combined_model/combined_model_embedding.py:250\u001b[39m, in \u001b[36mHierarchicalCombinedModelFixed.forward\u001b[39m\u001b[34m(self, sequences, use_hierarchy, true_labels)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# DNABERT embeddings (congelado)\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdnabert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[32m    253\u001b[39m z = \u001b[38;5;28mself\u001b[39m.encoder(emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/dna-neural-network-coi-/src/encoders_model/DNABERT_Embedder.py:71\u001b[39m, in \u001b[36mDNABERTEmbedder.forward\u001b[39m\u001b[34m(self, sequences)\u001b[39m\n\u001b[32m     68\u001b[39m inputs = {k: v.to(\u001b[38;5;28mself\u001b[39m.model.device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Get DNABERT outputs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m emb = outputs[\u001b[32m0\u001b[39m].mean(dim=\u001b[32m1\u001b[39m)   \u001b[38;5;66;03m# shape [B, H]\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m emb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/archives/bert_layers.py:609\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, attention_mask, position_ids, output_all_encoded_layers, masked_tokens_mask, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m     first_col_mask[:, \u001b[32m0\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    607\u001b[39m     subset_mask = masked_tokens_mask | first_col_mask\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubset_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubset_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m masked_tokens_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    616\u001b[39m     sequence_output = encoder_outputs[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/archives/bert_layers.py:447\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_all_encoded_layers, subset_mask)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subset_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    446\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer:\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m         hidden_states = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m                                     \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43malibi_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output_all_encoded_layers:\n\u001b[32m    455\u001b[39m             all_encoder_layers.append(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/archives/bert_layers.py:328\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, seqlen, subset_idx, indices, attn_mask, bias)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    307\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     bias: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    315\u001b[39m ) -> torch.Tensor:\n\u001b[32m    316\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass for a BERT layer, including both attention and MLP.\u001b[39;00m\n\u001b[32m    317\u001b[39m \n\u001b[32m    318\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m \u001b[33;03m        bias: None or (batch, heads, max_seqlen_in_batch, max_seqlen_in_batch)\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43msubset_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.mlp(attention_output)\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/archives/bert_layers.py:241\u001b[39m, in \u001b[36mBertUnpadAttention.forward\u001b[39m\u001b[34m(self, input_tensor, cu_seqlens, max_s, subset_idx, indices, attn_mask, bias)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    221\u001b[39m     input_tensor: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     bias: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    228\u001b[39m ) -> torch.Tensor:\n\u001b[32m    229\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass for scaled self-attention without padding.\u001b[39;00m\n\u001b[32m    230\u001b[39m \n\u001b[32m    231\u001b[39m \u001b[33;03m    Arguments:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m \u001b[33;03m        bias: None or (batch, heads, max_seqlen_in_batch, max_seqlen_in_batch)\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     self_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m subset_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output(index_first_axis(self_output, subset_idx),\n\u001b[32m    245\u001b[39m                            index_first_axis(input_tensor, subset_idx))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/archives/bert_layers.py:182\u001b[39m, in \u001b[36mBertUnpadSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, max_seqlen_in_batch, indices, attn_mask, bias)\u001b[39m\n\u001b[32m    180\u001b[39m bias_dtype = bias.dtype\n\u001b[32m    181\u001b[39m bias = bias.to(torch.float16)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m attention = \u001b[43mflash_attn_qkvpacked_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m attention = attention.to(orig_dtype)\n\u001b[32m    184\u001b[39m bias = bias.to(bias_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/autograd/function.py:576\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/archives/flash_attn_triton.py:1021\u001b[39m, in \u001b[36m_FlashAttnQKVPackedFunc.forward\u001b[39m\u001b[34m(ctx, qkv, bias, causal, softmax_scale)\u001b[39m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m qkv.stride(-\u001b[32m1\u001b[39m) != \u001b[32m1\u001b[39m:\n\u001b[32m   1020\u001b[39m     qkv = qkv.contiguous()\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m o, lse, ctx.softmax_scale = \u001b[43m_flash_attn_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m ctx.save_for_backward(qkv, o, lse, bias)\n\u001b[32m   1029\u001b[39m ctx.causal = causal\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/archives/flash_attn_triton.py:781\u001b[39m, in \u001b[36m_flash_attn_forward\u001b[39m\u001b[34m(q, k, v, bias, causal, softmax_scale)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m q.dtype == k.dtype == v.dtype, \u001b[33m'\u001b[39m\u001b[33mAll tensors must have the same type\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m q.dtype \u001b[38;5;129;01min\u001b[39;00m [torch.float16,\n\u001b[32m    780\u001b[39m                    torch.bfloat16], \u001b[33m'\u001b[39m\u001b[33mOnly support fp16 and bf16\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m q.is_cuda \u001b[38;5;129;01mand\u001b[39;00m k.is_cuda \u001b[38;5;129;01mand\u001b[39;00m v.is_cuda\n\u001b[32m    782\u001b[39m softmax_scale = softmax_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1.0\u001b[39m / math.sqrt(d)\n\u001b[32m    784\u001b[39m has_bias = bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = curriculum_trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    total_epochs=8,  \n",
    "    alpha=1.0,\n",
    "    beta=1.0,\n",
    "    lr=1e-3,\n",
    "    mixed_precision=True,\n",
    "    enable_early_stop=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0827a-0857-47d9-a8d6-89d03667da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = runner.evaluate_multitask(\n",
    "    test_loader=test_loader,\n",
    "    heads=taxon_order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a9e26-dc72-4681-a1c2-940e27faac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nğŸ“Š Resultados finales en test:\")\n",
    "print(f\"{'='*70}\")\n",
    "for taxon in taxon_order:\n",
    "    acc = test_results[taxon]['acc']\n",
    "    f1 = test_results[taxon]['f1']\n",
    "    print(f\"{taxon:12s} â†’ Acc: {acc:6.2%} | F1: {f1:.4f}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"\\nâœ… Script completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446975a5-0ce6-4570-93d4-b8843239d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': 10,  # Ãšltima Ã©poca completada\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'latent_dim': 128,\n",
    "        'max_length': 900,\n",
    "        'taxon_cols': taxon_cols,\n",
    "        'num_classes': {taxon: df[taxon].nunique() for taxon in taxon_cols}\n",
    "    }\n",
    "}, 'checkpoint_epoch10_completo.pt')\n",
    "\n",
    "print(\"âœ… Checkpoint completo guardado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ba3f0-82ed-41a3-b0a7-94426d9c7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GUARDAR MODELO COMPLETO =====\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# DespuÃ©s de terminar el entrenamiento\n",
    "checkpoint = {\n",
    "    'epoch': 10,  # o las Ã©pocas que completaste\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'latent_dim': 128,\n",
    "        'max_length': 900,\n",
    "        'taxon_cols': taxon_cols,\n",
    "        'num_classes': {taxon: df[taxon].nunique() for taxon in taxon_cols},\n",
    "        'dnabert_path': DNABERT_MODEL\n",
    "    },\n",
    "    # Opcional: guardar mapeos labelâ†’nombre\n",
    "    'label_mappings': {\n",
    "        taxon: {idx: name for idx, name in enumerate(sorted(df[taxon].unique()))}\n",
    "        for taxon in taxon_cols\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'modelo_coi_10epochs.pt')\n",
    "print(\"âœ… Modelo guardado en 'modelo_coi_10epochs.pt'\")\n",
    "\n",
    "# TambiÃ©n guardar historial separado para anÃ¡lisis\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Investigation 3.8",
   "language": "python",
   "name": "investigation_3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
